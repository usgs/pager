#!/usr/bin/env python

#stdlib imports
import argparse
import configparser
import os.path
import sys
import pprint
import locale
import json
import logging
import re
import glob
import traceback
import io
import datetime
import shutil
import textwrap
from urllib import request
import tempfile
import warnings
import socket

#third party imports
import matplotlib

#this allows us to have a non-interactive backend - essential on systems without a display
matplotlib.use('Agg')

import pandas as pd
import numpy as np
from mapio.shake import getHeaderData
from impactutils.comcat.query import ComCatInfo
from impactutils.transfer.factory import get_sender_class

#local imports
from losspager.models.exposure import Exposure
from losspager.models.econexposure import EconExposure
from losspager.models.emploss import EmpiricalLoss
from losspager.models.semimodel import SemiEmpiricalFatality
from losspager.models.growth import PopulationGrowth
from losspager.utils.country import Country
from losspager.utils.eventpath import get_event_folder
from losspager.utils.logger import PagerLogger
from losspager.utils.admin import PagerAdmin,split_event
from losspager.utils.exception import PagerException
from losspager.utils.datapath import get_data_path
from losspager.onepager.comment import get_impact_comments
from losspager.onepager.comment import get_structure_comment
from losspager.onepager.comment import get_secondary_comment
from losspager.onepager.comment import get_historical_comment
from losspager.onepager.onepager import create_onepager
from losspager.io.pagerdata import PagerData
from losspager.vis.impactscale import drawImpactScale
from losspager.vis.contourmap2 import draw_contour
from losspager.utils.config import read_config

COUNTRY = Country()
TIMEFMT = '%Y-%m-%d %H:%M:%S'
TSUNAMI_MAG_THRESH = 7.3

def _is_url(gridfile):
    try:
        fh = request.urlopen(gridfile)
        tdir = tempfile.mkdtemp()
        grid_file = os.path.join(tdir,'grid.xml')
        data = fh.read().decode('utf-8')
        fh.close()
        f = open(grid_file,'wt')
        f.write(data)
        f.close()
        return (True,grid_file)
    except:
        return (False,None)
    
def _check_pdl(gridfile,config):
    try:
        configfile = config['transfer']['pdl']['configfile']
        configbase,configname = os.path.split(configfile)
        lines = open(configfile,'rt').readlines()
        index_on = False
        for line in lines:
            if line.find('[indexer_listener_exec_storage]') > -1:
                index_on = True
                continue
            if index_on:
                if line.find('directory') > -1:
                    parts = line.split('=')
                    storage_dir = parts[1].strip()
                    storage_parts = storage_dir.split(os.path.sep)
                    grid_file = os.path.join(configbase,storage_parts,'shakemap',gridfile,'download','grid.xml')
                    if not os.path.isfile(grid_file):
                        return (False,None)
                    return (True,grid_file)
        return (False,None)
    except:
        return (False,None)
    
    
def _get_release_status(pargs,config,
                        fatmodel,fatdict,
                        ecomodel,ecodict,
                        shake_tuple,event_folder):
    #if we're not primary, it's released by default.
    #if our summary alert is green or yellow, it's released by default.
    #if the processing time is >= event time + 8(24?) hours, it's released by default.
    #if there is a "release" file in the event folder, it is released.
    #if the command line argument --release has been set, it is released.
    is_released = False

    #are we primary or secondary?
    if 'status' not in config:
        is_released = True
    else:
         if config['status'] == 'secondary':
             is_released = True

    #Are we at green or yellow
    fat_level = fatmodel.getAlertLevel(fatdict)
    eco_level = fatmodel.getAlertLevel(ecodict)
    if fat_level in ('green','yellow') and eco_level in ('green','yellow'):
        is_released = True

    #Are we past the release threshold?
    event_time = shake_tuple[1]['event_timestamp']
    threshold_hours = datetime.timedelta(seconds=config['release_threshold']*3600)
    time_threshold = event_time + threshold_hours
    if datetime.datetime.utcnow() > time_threshold:
        is_released = True

    #Is there a "release" file in the event folder?
    release_file = os.path.join(event_folder,'release')
    if os.path.isfile(release_file):
        is_released = True

    #Has the release option been set?
    if pargs.release:
        is_released = True

    return is_released

def message_pager(config,onepager_pdf,doc):
    if 'transfer' in config:
        if 'status' in config and config['status'] == 'primary':
            users = config['pager_team']
            sender = config['mail_from']
            host = config['mail_host']
            subject = '%s INTERNAL alert: %s' % (doc.summary_alert,doc.location)
            msg = '''This is an INTERNAL message notifying you of this %s alert.  You will receive 
            a second message with the pending alert.
            '''
            props = {'recipients':users,
                     'subject':subject,
                     'sender':sender,
                     'message':msg,
                     'smtp_servers':[host]}
            sender = EmailSender(props,local_files=[onepager_pdf])
            sender.send()
            
def transfer(config,pagerdata,authid,authsource,version_folder):
    #If system status is primary and transfer options are configured, transfer the output
    #directories using those options
    if 'status' in config and config['status'] == 'primary':
        if 'transfer' in config:
            if 'methods' in config['transfer']:
                print('Running transfer.')
                for method in config['transfer']['methods']:
                    if method not in config['transfer']:
                        sys.stderr.write('Method %s requested but not configured...Skipping.' % method)
                        continue
                    params = config['transfer'][method]
                    if 'remote_directory' in params:
                        vpath,vfolder = os.path.split(version_folder)
                        #append the event id and version folder to our pre-specified output directory
                        params['remote_directory'] = os.path.join(params['remote_directory'],authid,vfolder)
                    params['code'] = authid
                    params['eventsource'] = authsource
                    params['eventsourcecode'] = authid.replace(authsource,'')
                    params['magnitude'] = pagerdata.magnitude
                    params['latitude'] = pagerdata.latitude
                    params['longitude'] = pagerdata.longitude
                    params['depth'] = pagerdata.depth
                    params['eventtime'] = pagerdata.time
                    product_params = {'maxmmi':pagerdata.maxmmi,
                                      'alertlevel':pagerdata.summary_alert_pending}
                    sender_class = get_sender_class(method)
                    try:
                        if method == 'pdl':
                            sender = sender_class(properties=params,local_directory=version_folder,
                                                  product_properties=product_params)
                        else:
                            sender = sender_class(properties=params,local_directory=version_folder)
                        try:
                            nfiles,msg = sender.send()
                            print(msg)
                        except Exception as e:
                            print('Failed to send products via PDL: %s' % str(e))
                    except Exception as e:
                        sys.stderr.write('Could not send products via %s method - error "%s"' % (method,str(e)))
                        continue

def _get_pop_year(event_year,popyears):
    pop_year = None
    tmin = 10000000
    popfile = None
    for popdict in popyears:
        popyear = popdict['population_year']
        popgrid = popdict['population_grid']
        if not os.path.isfile(popgrid):
            print('Population grid file %s does not exist.' % popgrid)
            sys.exit(1)
        if abs(popyear-event_year) < tmin:
            tmin = abs(popyear-event_year)
            pop_year = popyear
            popfile = popgrid
    return (pop_year,popfile)

def get_pager_version(eventfolder):
    pager_version = 1
    if not os.path.isdir(eventfolder):
        os.makedirs(eventfolder)
        last_version = 0
    else:
        allfolders = glob.glob(os.path.join(eventfolder,'version.*'))
        allfolders.sort()
        if len(allfolders):
            base,last_folder = os.path.split(allfolders[-1])
            last_version = int(re.findall('\d+',last_folder)[0])
        else:
            last_version = 0
    return last_version + 1

def _draw_probs(fatmodel,fatdict,ecomodel,ecodict,version_folder):
    fatG = fatmodel.getCombinedG(fatdict)
    fat_probs = fatmodel.getProbabilities(fatdict,fatG)
    fat_figure = drawImpactScale(fat_probs,'fatality')

    ecoG = ecomodel.getCombinedG(ecodict)
    eco_probs = ecomodel.getProbabilities(ecodict,ecoG)
    eco_figure = drawImpactScale(eco_probs,'economic')

    fat_probs_file = os.path.join(version_folder,'alertfatal.pdf')
    fat_probs_file_png = os.path.join(version_folder,'alertfatal.png')
    fat_probs_file_small = os.path.join(version_folder,'alertfatal_small.png')
    fat_probs_file_smaller = os.path.join(version_folder,'alertfatal_smaller.png')
    
    eco_probs_file = os.path.join(version_folder,'alertecon.pdf')
    eco_probs_file_png = os.path.join(version_folder,'alertecon.png')
    eco_probs_file_small = os.path.join(version_folder,'alertecon_small.png')
    eco_probs_file_smaller = os.path.join(version_folder,'alertecon_smaller.png')
    
    fat_figure.savefig(fat_probs_file,bbox_inches='tight')
    fat_figure.savefig(fat_probs_file_png,bbox_inches='tight')
    fat_figure.savefig(fat_probs_file_small,bbox_inches='tight',dpi=57)
    fat_figure.savefig(fat_probs_file_smaller,bbox_inches='tight',dpi=35)
    
    eco_figure.savefig(eco_probs_file,bbox_inches='tight')
    eco_figure.savefig(eco_probs_file_png,bbox_inches='tight')
    eco_figure.savefig(eco_probs_file_small,bbox_inches='tight',dpi=57)
    eco_figure.savefig(eco_probs_file_smaller,bbox_inches='tight',dpi=35)
    return (fat_probs_file,eco_probs_file)

def _cancel(eventid,config):
    event_source,event_source_code = split_event(eventid)
    msg = ''
    if 'status' in config and config['status'] == 'primary':
        if 'transfer' in config:
            if 'methods' in config['transfer']:
                for method in config['transfer']['methods']:
                    if method not in config['transfer']:
                        sys.stderr.write('Method %s requested but not configured...Skipping.' % method)
                        continue
                    params = config['transfer'][method]
                    if 'remote_directory' in params:
                        vpath,vfolder = os.path.split(version_folder)
                        #append the event id and version folder to our pre-specified output directory
                        params['remote_directory'] = os.path.join(params['remote_directory'],authid,vfolder)
                    params['code'] = eventid
                    params['eventsource'] = event_source
                    params['eventsourcecode'] = event_source_code
                    params['magnitude'] = 0
                    params['latitude'] = 0
                    params['longitude'] = 0
                    params['depth'] = 0
                    params['eventtime'] = ''
                    sender_class = get_sender_class(method)
                    try:
                        if method == 'pdl':
                            sender = sender_class(properties=params,local_directory=version_folder,
                                                  product_properties=product_params)
                        else:
                            sender = sender_class(properties=params,local_directory=version_folder)
                        try:
                            msg += sender.cancel()
                        except Exception as e:
                            msg += 'Failed to send products via PDL: %s' % str(e)
                    except Exception as e:
                        msg += 'Could not send products via %s method - error "%s"' % (method,str(e))
                        
    return msg

def main(pargs,config):
    #get the users home directory
    homedir = os.path.expanduser('~')
    script_dir = os.path.dirname(os.path.abspath(__file__)) #where is this script?

    #handle cancel messages
    if pargs.cancel:
        #we presume that pargs.gridfile in this context is an event ID.
        msg = _cancel(pargs.gridfile,config)
        print(msg)
        sys.exit(0)
     
    #what kind of thing is gridfile?
    is_file = os.path.isfile(pargs.gridfile)
    is_url,url_gridfile = _is_url(pargs.gridfile)
    is_pdl,pdl_gridfile = _check_pdl(pargs.gridfile,config)
    if is_file:
        gridfile = pargs.gridfile
    elif is_url:
        gridfile = url_gridfile
    elif is_pdl:
        gridfile = pdl_gridfile
    else:
        print('ShakeMap Grid file %s does not exist.' % pargs.gridfile)
        sys.exit(1)
        
    
    pager_folder = os.path.join(homedir,config['output_folder'])
    pager_archive = os.path.join(homedir,config['archive_folder'])
    
    admin = PagerAdmin(pager_folder,pager_archive)
    
    plog = None
    if not pargs.debug:
        #stdout will now be logged as INFO, stderr will be logged as WARNING
        mail_host = config['mail_host']
        mail_from = config['mail_from']
        logfile = os.path.join(pager_folder,'pager.log')
        print('Writing content to %s' % logfile)
        plog = PagerLogger(logfile,redirect=True,
                           from_address=mail_from,
                           mail_host=mail_host)
        plog.addEmailHandler(config['developers']) #failover email alert system

    try:
        eid = None
        pager_version = None
        #get all the basic event information and print it, if requested
        shake_tuple = getHeaderData(gridfile)
        eid = shake_tuple[1]['event_id']
        etime = shake_tuple[1]['event_timestamp']
        if not len(eid):
            eid = shake_tuple[0]['event_id']
        network = shake_tuple[1]['event_network']
        if network == '':
            network = 'us'
        if not eid.startswith(network):
            eid = network + eid
        

        #Create a ComcatInfo object to hopefully tell us a number of things about this event
        try:
            ccinfo = ComCatInfo(eid)
            location = ccinfo.getLocation()
            tsunami = ccinfo.getTsunami()
            authid,allids = ccinfo.getAssociatedIds()
            authsource,othersources = ccinfo.getAssociatedSources()
        except: #fail over to what we can determine locally
            location = shake_tuple[1]['event_description']
            tsunami = shake_tuple[1]['magnitude'] >= TSUNAMI_MAG_THRESH
            authid = eid
            authsource = network
            allids = []

        #Check to see if user wanted to override default tsunami criteria
        if pargs.tsunami != 'auto':
            if pargs.tsunami == 'on':
                tsunami = True
            else:
                tsunami = False
            
        #check to see if this event is a scenario
        is_scenario = False
        shakemap_type = shake_tuple[0]['shakemap_event_type']
        if shakemap_type == 'SCENARIO':
            is_scenario = True
            if re.search('scenario',location.lower()) is None:
                location = 'Scenario '+location
            
        #create the event directory (if it does not exist), and start logging there
        print('Creating event directory')
        event_folder = admin.createEventFolder(authid,etime)
        pager_version = get_pager_version(event_folder)
        version_folder = os.path.join(event_folder,'version.%03d' % pager_version)
        os.makedirs(version_folder)
        event_logfile = os.path.join(version_folder,'event.log')
        if plog is not None:
            plog.switchToEventFileHandler(event_logfile)

        #Copy the grid.xml file to the version folder
        shutil.copy(gridfile,version_folder)

        #Check to see if the tsunami flag has been previously set
        tsunami_toggle = {'on':1,'off':0}
        tsunami_file = os.path.join(event_folder,'tsunami')
        if os.path.isfile(tsunami_file):
            tsunami = tsunami_toggle[open(tsunami_file,'rt').read().strip()]
            
        #get the rest of the event info
        etime = shake_tuple[1]['event_timestamp']
        elat = shake_tuple[1]['lat']
        elon = shake_tuple[1]['lon']
        edepth = shake_tuple[1]['depth']
        emag = shake_tuple[1]['magnitude']
        

        #get the year of the event
        event_year = shake_tuple[1]['event_timestamp'].year

        #find the population data collected most closely to the event_year
        pop_year,popfile = _get_pop_year(event_year,config['model_data']['population_data'])

        print('Population year: %i Population file: %s\n' % (pop_year,popfile))

        #Get exposure results
        print('Calculating population exposure.')
        isofile = config['model_data']['country_grid']
        expomodel = Exposure(popfile,pop_year,isofile)
        exposure = None
        exposure = expomodel.calcExposure(gridfile)

        #incidentally grab the country code of the epicenter
        numcode = expomodel._isogrid.getValue(elat,elon)
        cdict = Country().getCountry(int(numcode))
        if cdict is None:
            ccode = 'UK'
        else:
            ccode = cdict['ISO2']
        print('Country code at epicenter is %s' % ccode)

        #get fatality results, if requested
        print('Calculating empirical fatalities.')
        fatmodel = EmpiricalLoss.fromDefaultFatality()
        fatdict = fatmodel.getLosses(exposure)

        #get economic results, if requested
        print('Calculating economic exposure.')
        econexpmodel = EconExposure(popfile,pop_year,isofile)
        ecomodel = EmpiricalLoss.fromDefaultEconomic()
        econexposure = econexpmodel.calcExposure(gridfile)
        ecodict = ecomodel.getLosses(econexposure)
        shakegrid = econexpmodel.getShakeGrid()

        #Get semi-empirical losses, if requested
        print('Calculating semi-empirical fatalities.')
        urbanfile = config['model_data']['urban_rural_grid']
        if not os.path.isfile(urbanfile):
            raise PagerException('Urban-rural grid file %s does not exist.' % urbanfile)

        semi = SemiEmpiricalFatality.fromDefault()
        semi.setGlobalFiles(popfile,pop_year,urbanfile,isofile)
        semiloss,resfat,nonresfat = semi.getLosses(gridfile)

        #get all of the other components of PAGER
        print('Getting all comments.')
        #get the fatality and economic comments
        impact1,impact2 = get_impact_comments(fatdict,ecodict,econexposure,event_year,ccode)
        #get comment describing vulnerable structures in the region.
        struct_comment = get_structure_comment(resfat,nonresfat,semi)
        #get the comment describing historic secondary hazards
        secondary_comment = get_secondary_comment(elat,elon,emag)
        #get the comment describing historical comments in the region
        historical_comment = get_historical_comment(elat,elon,emag,exposure,fatdict)

        #generate the probability plots
        print('Drawing probability plots.')
        fat_probs_file,eco_probs_file = _draw_probs(fatmodel,fatdict,
                                                    ecomodel,ecodict,
                                                    version_folder)

        #generate the exposure map
        exposure_base = os.path.join(version_folder,'exposure')
        print('Generating exposure map...')
        oceanfile = config['model_data']['ocean_vectors']
        oceangrid = config['model_data']['ocean_grid']
        cityfile = config['model_data']['city_file']
        pdf_file,png_file,mapcities = draw_contour(gridfile,popfile,
                                         oceanfile,oceangrid,cityfile,
                                         exposure_base,is_scenario=is_scenario)
        print('Generated exposure map %s' % pdf_file)

        #figure out whether this event has been "released".
        is_released = _get_release_status(pargs,config,
                                          fatmodel,fatdict,
                                          ecomodel,ecodict,
                                          shake_tuple,event_folder)
        
        
        
        #Create a data object to encapsulate everything we know about the PAGER
        #results, and then serialize that to disk in the form of a number of JSON files.
        print('Making PAGER Data object.')
        doc = PagerData()
        timezone_file = config['model_data']['timezones_file']
        elapsed = pargs.elapsed
        doc.setInputs(shakegrid,timezone_file,pager_version,shakegrid.getEventDict()['event_id'],
                      authid,tsunami,location,is_released,elapsed=elapsed)
        print('Setting inputs.')
        doc.setExposure(exposure,econexposure)
        print('Setting exposure.')
        doc.setModelResults(fatmodel,ecomodel,
                            fatdict,ecodict,
                            semiloss,resfat,nonresfat)
        print('Setting comments.')
        doc.setComments(impact1,impact2,struct_comment,historical_comment,secondary_comment)
        print('Setting map info.')
        doc.setMapInfo(cityfile,mapcities)
        print('Validating.')
        doc.validate()
        json_folder = os.path.join(version_folder,'json')
        os.makedirs(json_folder)
        print('Saving output to JSON.')
        doc.saveToJSON(json_folder)
        print('Saving output to XML.')
        doc.saveToLegacyXML(version_folder)

        print('Creating onePAGER pdf...')
        onepager_pdf,error = create_onepager(doc,version_folder)
        if onepager_pdf is None:
            raise PagerException('Could not create onePAGER output: \n%s' % error)

        #copy the contents.xml file to the version folder
        contentsfile = get_data_path('contents.xml')
        if contentsfile is None:
            raise PagerException('Could not find contents.xml file.')
        shutil.copy(contentsfile,version_folder)

        #send pdf as attachment to internal team of PAGER users
        message_pager(config,onepager_pdf,doc)
        
        #run transfer, as appropriate and as specified by config
        transfer(config,doc,authid,authsource,version_folder)

        if plog is not None:
            plog.stopLogging()
        print('Created onePAGER pdf %s' % onepager_pdf)
        
        print('Done.')
        #There is a weird error that happens (sometimes) after calling exit.  I *think* it has to do with
        #Python trying to delete an object that happens to be None.  It's not clear where it's coming from
        #and as it happens after the code completes, I'm not terribly concerned about it other than
        #confusion for users.  Hence the try block around sys.exit().
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                sys.exit(0)
        except:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                sys.exit(0)
    except Exception as e:
        if plog is not None:
            f = io.StringIO()
            traceback.print_exc(file=f)
            msg = e
            msg = '%s\n %s' % (str(msg),f.getvalue())
            hostname = socket.gethostname()
            msg = msg + '\n' + 'Error occurred on %s\n' % (hostname)
            if gridfile is not None:
                msg = msg + '\n' + 'Error on file: %s\n' % (gridfile)
            if eid is not None:
                msg = msg + '\n' + 'Error on event: %s\n' % (eid)
            if pager_version is not None:
                msg = msg + '\n' + 'Error on version: %i\n' % (pager_version)
            f.close()
            if plog is not None:
                plog.scream(msg)
                plog.stopLogging()
            print('Sent error to email')
            
        else:
            sys.stderr.write(str(e)+'\n')
            traceback.print_exc()
        print('Something bad happened!')
        sys.exit(1)
if __name__ == '__main__':
    desc = '''Run PAGER loss models and create all PAGER products.

    This program presumes that you have a configuration file in ~/.losspager/config.yml.
    
    Example usage:
    %(prog)s grid
     , where grid can be either:
      - A path to a local ShakeMap grid.xml file.
      - An event ID (i.e., us2010abcd), which (on a primary system) will find the most recently PDL-downloaded grid file.
      - A url (http://earthquake.usgs.gov/realtime/product/shakemap/us10007tas/us/1484425631405/download/grid.xml)

    On either a primary or secondary system, the PAGER models will be run, and will generate maps, figures, JSON files,
    XML files, and a onePAGER PDF file in the output directory specified in the above config file.

    On primary systems that have had transfer modules configured, this output will be transferred (usually via PDL) 
    to remote systems.

    On secondary systems, the PAGER run will end with generating the content.

    To send a cancel message for an event (does nothing in laptop configuration):
    %(prog)s --cancel eventid
    '''
    parser = argparse.ArgumentParser(description=desc,formatter_class=argparse.RawTextHelpFormatter)
    gridhelp = '''gridfile can be either:
      - A path to a local ShakeMap grid.xml file.
      - An event ID (i.e., us2010abcd), which (on a primary system) will find the most recently PDL-downloaded grid file.
      - A url (http://earthquake.usgs.gov/realtime/product/shakemap/us10007tas/us/1484425631405/download/grid.xml)
    '''
    parser.add_argument('gridfile',
                        help=textwrap.dedent(gridhelp))
    parser.add_argument('-d','--debug', action='store_true',
                        default=False, help='Print debug information (mostly useful to developers)')
    parser.add_argument('--release', action='store_true',
                        default=False, help='Set release status to True (override other criteria)')
    parser.add_argument('--cancel', action='store_true',
                        default=False, help='Send a cancel message using any configured transfer methods')
    parser.add_argument('--tsunami', choices=['on','off','auto'],
                        default='auto', help='Set tsunami flag to True/False (override other criteria), or use default criteria')
    parser.add_argument('--elapsed', type=int, metavar='ELAPSED',
                        help='Override calculated elapsed time with minutes.  Usually used for scenarios.')
    
    
    
    args = parser.parse_args()

    #read in the config - should be in a predictable place - this will error out if not.
    config = read_config()
    
    #Make sure model_data section exists
    try:
        config['model_data']['population_data'][0]['population_year']
        config['model_data']['population_data'][0]['population_grid']
        os.path.isfile(config['model_data']['country_grid'])
        os.path.isfile(config['model_data']['urban_rural_grid'])
    except:
        errmsg = 'Config file %s is missing some or all of the required information.  See the help for the required format.\n'
        sys.stderr.write(errmsg)
        sys.exit(1)
    
    main(args,config)
